---
title: Definition of Ready and Done
weight: 3
---

# <%= current_page.data.title %>

### Definition of Ready

A more comprehensive version of Definition of Ready and Done can be found here: [Definition of Ready and Done.](https://tools.hmcts.net/confluence/display/PMOR/Definition+of+Ready+and+Done). This page focuses purely on the QA piece.

Following the backlog refinement session and prior to the planning & estimation session the QA should ensure the following is completed to meet the Definition of Ready for a Story:

* <strong>Acceptance test criteria</strong> have been written and discussed (with at minimum) the BA and PO and preferably a Dev as well.
* The QA has analysed what level of test coverage is required for the Story:
  * <strong>Unit/component</strong> test coverage (Mocha/Chai; Junit etc)
  * <strong>Integration</strong> and <strong>contract</strong> testing coverage (RestAssured; Spring) - Do you integrate with any new services / APIs?  Do you need to update any stale data?
  * <strong>Functional automation</strong> coverage (Codecept/Protractor). What coverage will the automation test provide?
  * <strong>Cross browser</strong> – Do you need to extend the cross browser tests or add more browsers/browser versions to the suite?
  * <strong>Performance</strong> – Do you need to extend the in-sprint performance test scenarios? Is any formal performance testing required?
  * <strong>Accessibility</strong> - Beyond Pa11y is there a requirement to run any further accessibility tests via assistive technology tools or additional w3c compliance checkers?
  * <strong>Manual testing</strong> - What percentage of testing needs to be conducted manually through exploratory or scripted testing?
  * <strong>Manual Integration testing</strong> - Are your changes going to interact with any upstream/downstream systems? Consider planning & scripting requirements outside your system boundaries.
  * <strong>Devices</strong> - What level of device testing is required? Do you need to test with Sauce Labs, Responsive Web Design checkers/emulators or with real devices?
  * <strong>Security</strong> - Will the daily OWASP check be adequate or is more detailed security testing required using tools such as Burpsuite and BlackDuck? Is an ITHC required?
  * <strong>UAT</strong> and <strong>BAT</strong> - is there a requirement for this Story to be story tested or form part of an e2e scenario test with users (via internal or lab sessions) or with business stakeholders?
  * <strong>Reporting</strong> -  Does it require any additional reporting to be generated (i.e. via Jenkins)
  * <strong>Test Harness availability</strong> - If the story is dependant on Common components, check whether a test harness is required/available for your story to integrate with CC 
  
### During the development process the QA should:

* Pre-story huddle with a BA/PO and the developer picking up the Story.
* Post story huddle with a BA/PO and the developer checking in the code for the Story.
* View all Pull Requests where possible checking for automated test coverage.

### Definition of Done

In order for a Story to be considered complete the QA should check the following with the team prior to sign off of the story by a PO:

* Meets unit test code coverage (>80%) - via SonarQube
* All acceptance criteria have been tested.
* All automated tests identified as part of the required test coverage have been created, executed and passed (via Jenkins).
* All required manual tests have been executed and passed. 
* All manual Zephyr test suites have been updated (Regression suite and/or Project specific Exploratory suite)
* Ensure sign off with a PO.

#### Reference:

[Definition of Ready and Done](https://tools.hmcts.net/confluence/display/PMOR/Definition+of+Ready+and+Done)

[QA Activities](https://tools.hmcts.net/confluence/display/RQA/Reform+-+High+Level+Test+Strategy#QA%2520activities)

[Three Amigos](https://www.agilealliance.org/glossary/three-amigos/)
